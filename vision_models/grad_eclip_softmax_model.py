# numpy
import numpy as np

# modeling
from vision_models.base_model import BaseModel
import torch
import torch.nn.functional as F
import torchvision.transforms as T
from torchvision.transforms import Compose, Resize, ToTensor, Normalize, InterpolationMode
import clip
from PIL import Image

# typing
from typing import List, Optional


class GradEclipSoftmaxModel(BaseModel):
    def __init__(self, model_name="ViT-B/16", device="cuda", temperature=1.0):
        super().__init__()
        self.device = device
        self.feature_dim = 1  # Grad-ECLIP heatmapì€ 1ì°¨ì›
        self.temperature = temperature  # Softmax temperature scaling
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        self.clip_model, self.preprocess = clip.load(model_name, device=device)
        # CLIP ëª¨ë¸ì„ float32ë¡œ ìœ ì§€ (half precision ë¬¸ì œ í•´ê²°)
        self.clip_model = self.clip_model.float()
        self.clip_inres = self.clip_model.visual.input_resolution  # ì´ë¯¸ì§€ í•´ìƒë„
        self.clip_ksize = self.clip_model.visual.conv1.kernel_size  # íŒ¨ì¹˜ ì‚¬ì´ì¦ˆ (16 x 16)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self._transform = Compose([
            ToTensor(),
            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
        ])
        
        print(f"[Grad-ECLIP-Softmax] CLIP resolution: {self.clip_inres}")
        print(f"[Grad-ECLIP-Softmax] CLIP kernel size: {self.clip_ksize}")
        print(f"[Grad-ECLIP-Softmax] Feature dimension: {self.feature_dim}")
        print(f"[Grad-ECLIP-Softmax] Softmax temperature: {self.temperature}")

    def imgprocess(self, img, patch_size=[16, 16], scale_factor=1):
        """ì´ë¯¸ì§€ë¥¼ CLIP ëª¨ë¸ì— ë§ê²Œ ì „ì²˜ë¦¬"""
        w, h = img.size
        ph, pw = patch_size
        nw = int(w * scale_factor / pw + 0.5) * pw
        nh = int(h * scale_factor / ph + 0.5) * ph

        ResizeOp = Resize((nh, nw), interpolation=InterpolationMode.BICUBIC)
        img = ResizeOp(img).convert("RGB")
        return self._transform(img)

    def attention_layer(self, q, k, v, num_heads=1):
        """Compute 'Scaled Dot Product Attention'"""
        tgt_len, bsz, embed_dim = q.shape
        head_dim = embed_dim // num_heads
        scaling = float(head_dim) ** -0.5
        q = q * scaling

        q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
        attn_output_weights = torch.bmm(q, k.transpose(1, 2))
        attn_output_weights = F.softmax(attn_output_weights, dim=-1)
        attn_output_heads = torch.bmm(attn_output_weights, v)
        assert list(attn_output_heads.size()) == [bsz * num_heads, tgt_len, head_dim]
        attn_output = attn_output_heads.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, -1)
        attn_output_weights = attn_output_weights.sum(dim=1) / num_heads
        return attn_output, attn_output_weights

    def clip_encode_dense(self, x, n):
        """CLIP ëª¨ë¸ì˜ dense features ì¶”ì¶œ"""
        vision_width = self.clip_model.visual.transformer.width
        vision_heads = vision_width // 64

        # modified from CLIP
        # ì…ë ¥ì„ float32ë¡œ ìœ ì§€
        if x.dtype != torch.float32:
            x = x.float()
        x = self.clip_model.visual.conv1(x)
        feah, feaw = x.shape[-2:]

        x = x.reshape(x.shape[0], x.shape[1], -1)
        x = x.permute(0, 2, 1)
        class_embedding = self.clip_model.visual.class_embedding.to(x.dtype)
        x = torch.cat([class_embedding + torch.zeros(x.shape[0], 1, x.shape[-1]).to(x), x], dim=1)

        # scale position embedding as the image w-h ratio
        pos_embedding = self.clip_model.visual.positional_embedding.to(x.dtype)
        tok_pos, img_pos = pos_embedding[:1, :], pos_embedding[1:, :]
        pos_h = self.clip_inres // self.clip_ksize[0]
        pos_w = self.clip_inres // self.clip_ksize[1]
        assert img_pos.size(0) == (pos_h * pos_w), f"the size of pos_embedding ({img_pos.size(0)}) does not match resolution shape pos_h ({pos_h}) * pos_w ({pos_w})"
        img_pos = img_pos.reshape(1, pos_h, pos_w, img_pos.shape[1]).permute(0, 3, 1, 2)
        img_pos = torch.nn.functional.interpolate(img_pos, size=(feah, feaw), mode='bicubic', align_corners=False)
        img_pos = img_pos.reshape(1, img_pos.shape[1], -1).permute(0, 2, 1)
        pos_embedding = torch.cat((tok_pos[None, ...], img_pos), dim=1)
        x = x + pos_embedding
        x = self.clip_model.visual.ln_pre(x)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = torch.nn.Sequential(*self.clip_model.visual.transformer.resblocks[:-n])(x)

        attns = []
        atten_outs = []
        vs = []
        qs = []
        ks = []
        for TR in self.clip_model.visual.transformer.resblocks[-n:]:
            x_in = x
            x = TR.ln_1(x_in)
            linear = torch._C._nn.linear
            q, k, v = linear(x, TR.attn.in_proj_weight, TR.attn.in_proj_bias).chunk(3, dim=-1)
            attn_output, attn = self.attention_layer(q, k, v, 1)  # vision_heads=1
            attns.append(attn)
            atten_outs.append(attn_output)
            vs.append(v)
            qs.append(q)
            ks.append(k)

            x_after_attn = linear(attn_output, TR.attn.out_proj.weight, TR.attn.out_proj.bias)
            x = x_after_attn + x_in
            x = x + TR.mlp(TR.ln_2(x))

        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.clip_model.visual.ln_post(x)
        x = x @ self.clip_model.visual.proj
        return x, x_in, vs, qs, ks, attns, atten_outs, (feah, feaw)

    def sim_qk(self, q, k):
        """Query-Key similarity ê³„ì‚°"""
        q_cls = F.normalize(q[:1, 0, :], dim=-1)
        k_patch = F.normalize(k[1:, 0, :], dim=-1)

        cosine_qk = (q_cls * k_patch).sum(-1)
        cosine_qk_max = cosine_qk.max(dim=-1, keepdim=True)[0]
        cosine_qk_min = cosine_qk.min(dim=-1, keepdim=True)[0]
        cosine_qk = (cosine_qk - cosine_qk_min) / (cosine_qk_max - cosine_qk_min)
        return cosine_qk

    def grad_eclip(self, c, qs, ks, vs, attn_outputs, map_size):
        """Grad-ECLIP heatmap ìƒì„± + Softmax"""
        # gradient on last attention output
        tmp_maps = []
        for q, k, v, attn_output in zip(qs, ks, vs, attn_outputs):
            grad = torch.autograd.grad(
                c,
                attn_output,
                retain_graph=True)[0]

            grad_cls = grad[:1, 0, :]
            v_patch = v[1:, 0, :]
            cosine_qk = self.sim_qk(q, k).reshape(-1)
            tmp_maps.append((grad_cls * v_patch * cosine_qk[:, None]).sum(-1))

        emap = F.relu_(torch.stack(tmp_maps, dim=0)).sum(0)
        
        # ğŸ”‘ í•µì‹¬: ê°œì„ ëœ Softmax ì ìš©!
        # 1. Raw values ì €ì¥ (ë””ë²„ê¹…ìš©)
        raw_min, raw_max = emap.min(), emap.max()
        
        # 2. Heatmapì„ flattení•˜ì—¬ ì „ì²´ì— softmax ì ìš©
        emap_flat = emap.flatten()
        emap_softmax = torch.softmax(emap_flat / self.temperature, dim=0)
        emap = emap_softmax.reshape(*map_size)
        
        # 3. ì‹œê°í™” ê°œì„ ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ (ì„ íƒì )
        # Softmax ê°’ë“¤ì„ [0, 1] ë²”ìœ„ì—ì„œ ë” ì˜ ë³´ì´ë„ë¡ ì¡°ì •
        if emap.max() > 0:
            # ìƒìœ„ 5% ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
            top_5_percent = torch.quantile(emap.flatten(), 0.95)
            if top_5_percent > 0:
                emap_scaled = torch.clamp(emap / top_5_percent, 0, 1)
            else:
                emap_scaled = emap
        else:
            emap_scaled = emap
        
        # 4. ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"[Grad-ECLIP-Softmax] Raw range: [{raw_min:.3f}, {raw_max:.3f}]")
        print(f"[Grad-ECLIP-Softmax] Softmax range: [{emap.min():.6f}, {emap.max():.6f}], Sum: {emap.sum():.6f}")
        print(f"[Grad-ECLIP-Softmax] Scaled range: [{emap_scaled.min():.6f}, {emap_scaled.max():.6f}]")
        print(f"[Grad-ECLIP-Softmax] Temperature: {self.temperature}")
        
        # 5. ì‹œê°í™”ìš©ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëœ ë²„ì „ ë°˜í™˜
        return emap_scaled

    def get_image_features(self, image: np.ndarray, query_text: str = "toilet") -> torch.Tensor:
        """
        Grad-ECLIP heatmap ìƒì„± (Softmax ì ìš©)
        Args:
            image: (B, C, H, W) ë˜ëŠ” (C, H, W) ë˜ëŠ” (H, W, C) í˜•íƒœì˜ ì´ë¯¸ì§€
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (ì˜ˆ: "toilet")
        Returns:
            heatmap: (1, H, W) í˜•íƒœì˜ í™•ë¥ ì  relevance map
        """
        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥ (resizeìš©)
        original_shape = image.shape
        
        # ì´ë¯¸ì§€ í˜•íƒœ ì •ê·œí™” - ì™„ì „íˆ ìƒˆë¡œìš´ ë°°ì—´ ìƒì„±
        if len(image.shape) == 4:  # (B, C, H, W)
            img_array = np.array(image[0], copy=True, dtype=np.uint8)  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©í•˜ê³  ë³µì‚¬
            h, w = original_shape[2], original_shape[3]  # (B, C, H, W)ì—ì„œ H, W ì¶”ì¶œ
        elif len(image.shape) == 3:  # (C, H, W) ë˜ëŠ” (H, W, C)
            if image.shape[0] == 3:  # (C, H, W)
                img_array = np.array(image.transpose(1, 2, 0), copy=True, dtype=np.uint8)  # (H, W, C)ë¡œ ë³€í™˜í•˜ê³  ë³µì‚¬
                h, w = original_shape[1], original_shape[2]  # (C, H, W)ì—ì„œ H, W ì¶”ì¶œ
            else:  # (H, W, C) í˜•íƒœëŠ” ê·¸ëŒ€ë¡œ ë³µì‚¬
                img_array = np.array(image, copy=True, dtype=np.uint8)
                h, w = original_shape[0], original_shape[1]  # (H, W, C)ì—ì„œ H, W ì¶”ì¶œ
        else:
            raise ValueError(f"Unexpected image shape: {image.shape}")
        
        # numpy arrayë¥¼ PIL Imageë¡œ ë³€í™˜
        if img_array.dtype != np.uint8:
            # ì •ê·œí™”ëœ ê°’ [0,1]ì„ [0,255]ë¡œ ë³€í™˜
            if img_array.max() <= 1.0:
                img_array = (img_array * 255).astype(np.uint8)
            else:
                img_array = img_array.astype(np.uint8)
        
        # PIL Image ìƒì„±
        img_pil = Image.fromarray(img_array)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_preprocessed = self.imgprocess(img_pil).to(self.device).unsqueeze(0)
        
        # CLIP dense features ì¶”ì¶œ
        outputs, last_feat, vs, qs, ks, attns, atten_outs, map_size = self.clip_encode_dense(img_preprocessed, n=1)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        text_processed = clip.tokenize([query_text]).to(self.device)
        text_embedding = self.clip_model.encode_text(text_processed)
        text_embedding = F.normalize(text_embedding, dim=-1)
        
        # ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ì˜ cosine similarity ê³„ì‚°
        img_embedding = F.normalize(outputs[:, 0], dim=-1)
        cosine = (img_embedding @ text_embedding.T)[0]
        
        # Grad-ECLIP heatmap ìƒì„± (Softmax ì ìš©!)
        heatmap = self.grad_eclip(cosine[0], qs, ks, vs, atten_outs, map_size)
        
        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ resize (ì €ì¥ëœ h, w ì‚¬ìš©)
        resize = T.Resize((h, w))
        heatmap = resize(heatmap.unsqueeze(0)).squeeze(0)
        
        # (H, W) -> (1, H, W) í˜•íƒœë¡œ ë°˜í™˜ (OneMapì—ì„œ ì‚¬ìš©í•  í˜•íƒœ)
        # heatmapì€ ì´ë¯¸ self.deviceì— ìˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì´ë™ ë¶ˆí•„ìš”
        heatmap_result = heatmap.unsqueeze(0)
        
        print(f"[Grad-ECLIP-Softmax] Final heatmap - Shape: {heatmap_result.shape}, Range: [{heatmap_result.min():.6f}, {heatmap_result.max():.6f}]")
        
        return heatmap_result

    def get_text_features(self, texts: List[str]) -> torch.Tensor:
        """
        í…ìŠ¤íŠ¸ features ì¶”ì¶œ (BaseModel ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ìš©)
        Grad-ECLIPì—ì„œëŠ” ì‹¤ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        """
        text_processed = clip.tokenize(texts).to(self.device)
        text_embedding = self.clip_model.encode_text(text_processed)
        return F.normalize(text_embedding, dim=-1)

    def compute_similarity(self, image_feats: torch.Tensor, text_feats: torch.Tensor) -> torch.Tensor:
        """
        ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ features ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (BaseModel ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ìš©)
        Grad-ECLIPì˜ ê²½ìš° ì´ë¯¸ì§€ featuresê°€ ì´ë¯¸ heatmapì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        # Grad-ECLIP heatmapì€ ì´ë¯¸ relevance scoreì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(image_feats.shape) == 3:  # (1, H, W)
            return image_feats.squeeze(0)  # (H, W)
        else:
            return image_feats

    def eval(self):
        """í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •"""
        self.clip_model.eval()
        return self


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = GradEclipSoftmaxModel(model_name="ViT-B/16", device=device, temperature=1.0)
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
    test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    
    # ì´ë¯¸ì§€ features ì¶”ì¶œ (Grad-ECLIP-Softmax heatmap)
    heatmap = model.get_image_features(test_image, "toilet")
    print(f"Softmax Heatmap shape: {heatmap.shape}")
    print(f"Softmax Heatmap range: {heatmap.min():.6f} ~ {heatmap.max():.6f}")
    print(f"Softmax Heatmap sum: {heatmap.sum():.6f}")
    
    # OneMapì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœì¸ì§€ í™•ì¸
    print(f"OneMap feature_dim=1ê³¼ í˜¸í™˜: {heatmap.shape[0] == 1}")